{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f211c6a7",
   "metadata": {},
   "source": [
    "# üì∞ Challenge 2: Fake News Detection AI Project - Educational Notebook\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/BridgingAISocietySummerSchools/Coding-Project/blob/main/challenge_2/notebooks/educational_notebook_2.ipynb)\n",
    "\n",
    "## üéØ Theme: AI for Combating Misinformation and Promoting Media Literacy\n",
    "\n",
    "Welcome to Challenge 2! You'll build an intelligent system to detect fake news articles using Natural Language Processing and machine learning. This challenge explores how AI can help combat misinformation while considering the complex challenges of bias, context, and freedom of expression.\n",
    "\n",
    "## üìñ What You'll Learn\n",
    "- **Text Preprocessing**: Clean and prepare news article text for analysis\n",
    "- **Feature Extraction**: Convert text into numerical features using TF-IDF\n",
    "- **Binary Classification**: Distinguish between real and fake news articles\n",
    "- **Model Evaluation**: Assess performance with precision, recall, and F1-score\n",
    "- **Pattern Recognition**: Identify linguistic patterns in misinformation\n",
    "- **Ethical Considerations**: Navigate the complexities of automated content moderation\n",
    "\n",
    "## \udcf0 Dataset Overview\n",
    "You'll work with real news articles loaded directly from GitHub:\n",
    "- **Real News**: Authentic articles from reputable news sources (`True.csv`)\n",
    "- **Fake News**: Identified misinformation articles (`Fake.csv`)\n",
    "- **Sample Size**: 40,000+ articles total\n",
    "- **Features**: Article titles, content, subjects, and publication dates\n",
    "- **Source**: Curated collection from various news outlets\n",
    "- **Access**: Automatically loaded from GitHub repository\n",
    "\n",
    "## üöÄ Challenge Roadmap\n",
    "Follow these steps to build your fake news detector:\n",
    "\n",
    "1. **üìä Data Exploration**: Understand the structure and patterns in news data\n",
    "2. **üßπ Text Preprocessing**: Clean and standardize article text\n",
    "3. **üîç Feature Engineering**: Extract meaningful features from text\n",
    "4. **ü§ñ Model Training**: Build and train classification models\n",
    "5. **üìà Model Evaluation**: Assess performance and understand predictions\n",
    "6. **üî¨ Pattern Analysis**: Discover what makes news \"fake\" vs \"real\"\n",
    "7. **üí≠ Ethical Reflection**: Consider implications of automated fact-checking\n",
    "\n",
    "---\n",
    "\n",
    "## üí° **Key Insight**: \n",
    "Fake news detection is not just a technical challenge - it involves complex questions about truth, bias, context, and the role of AI in information ecosystems. Building these tools requires careful consideration of their societal impact.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ö†Ô∏è **Important Considerations**:\n",
    "- **No system is perfect**: AI can help flag suspicious content but shouldn't be the sole arbiter of truth\n",
    "- **Context matters**: The same facts can be presented with different biases\n",
    "- **Evolution of deception**: Bad actors constantly adapt to evade detection systems\n",
    "- **Human oversight**: Critical decisions about information should involve human judgment\n",
    "\n",
    "---\n",
    "\n",
    "### Task 1: Load and Explore the Dataset\n",
    "\n",
    "**üéØ Goal**: Understand your news data and the challenge of distinguishing real from fake content\n",
    "\n",
    "**üìù What to do**:\n",
    "- Load the dataset directly from GitHub repository\n",
    "- Examine article structure and content\n",
    "- Analyze the distribution of real vs. fake news\n",
    "- Compare sample articles to identify potential patterns\n",
    "- Look for obvious linguistic or structural differences\n",
    "\n",
    "**üí° Key Questions to Explore**:\n",
    "- What makes an article \"fake\" vs. \"real\"?\n",
    "- Are there obvious patterns in language, tone, or structure?\n",
    "- How might the definition of \"fake news\" affect our approach?\n",
    "- What challenges might arise in real-world deployment?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57d8eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Load the dataset from GitHub repository\n",
    "print(\"Loading fake news dataset from GitHub...\")\n",
    "\n",
    "# GitHub raw URLs for the dataset files\n",
    "fake_url = \"https://raw.githubusercontent.com/BridgingAISocietySummerSchools/Coding-Project/main/challenge_2/data/Fake.csv\"\n",
    "true_url = \"https://raw.githubusercontent.com/BridgingAISocietySummerSchools/Coding-Project/main/challenge_2/data/True.csv\"\n",
    "\n",
    "try:\n",
    "    # Load fake news articles\n",
    "    fake_df = pd.read_csv(fake_url)\n",
    "    fake_df['label'] = 0  # 0 for fake news\n",
    "    print(f\"‚úÖ Loaded {len(fake_df)} fake news articles\")\n",
    "    \n",
    "    # Load true news articles  \n",
    "    true_df = pd.read_csv(true_url)\n",
    "    true_df['label'] = 1  # 1 for real news\n",
    "    print(f\"‚úÖ Loaded {len(true_df)} real news articles\")\n",
    "    \n",
    "    # Combine datasets\n",
    "    df = pd.concat([fake_df, true_df], ignore_index=True)\n",
    "    \n",
    "    print(f\"\\nüìä Combined dataset shape: {df.shape}\")\n",
    "    print(f\"üìä Total articles: {len(df)}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading data: {e}\")\n",
    "    print(\"üí° Trying to load from local data folder as fallback...\")\n",
    "    try:\n",
    "        df = pd.read_csv(\"../data/fake_news_dataset.csv\")\n",
    "        print(\"‚úÖ Loaded from local data folder\")\n",
    "    except:\n",
    "        print(\"‚ùå Could not load data from local folder either\")\n",
    "        raise\n",
    "\n",
    "print(\"\\nüîç Dataset Overview:\")\n",
    "print(\"First few rows:\")\n",
    "display_cols = ['title', 'text', 'label'] if 'title' in df.columns else df.columns[:3].tolist()\n",
    "print(df[display_cols].head())\n",
    "\n",
    "print(\"\\nüìà Label distribution:\")\n",
    "label_counts = df['label'].value_counts()\n",
    "print(f\"Real news (1): {label_counts.get(1, 0)}\")\n",
    "print(f\"Fake news (0): {label_counts.get(0, 0)}\")\n",
    "\n",
    "print(\"\\nüìã Column information:\")\n",
    "print(f\"Columns: {list(df.columns)}\")\n",
    "print(f\"Data types: {df.dtypes.to_dict()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c9ad7e",
   "metadata": {},
   "source": [
    "### Task 2: Text Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89e6475",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download required NLTK data\n",
    "try:\n",
    "    stopwords.words('english')\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove special characters and digits\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    # Remove stopwords\n",
    "    text = ' '.join([word for word in text.split() if word not in stop_words])\n",
    "    return text\n",
    "\n",
    "# YOUR_CODE_HERE: Apply preprocessing to the text column\n",
    "df['processed_text'] = df['text'].apply(preprocess_text)\n",
    "\n",
    "print(\"Text preprocessing completed!\")\n",
    "print(df[['text', 'processed_text']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68fb835",
   "metadata": {},
   "source": [
    "### Task 3: Feature Extraction and Model Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6360d35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR_CODE_HERE: Split the data into training and testing sets\n",
    "X = df['processed_text']\n",
    "y = df['label']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# YOUR_CODE_HERE: Create TF-IDF features\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "# YOUR_CODE_HERE: Train a Naive Bayes classifier\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "print(\"Model training completed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d66d48",
   "metadata": {},
   "source": [
    "### Task 4: Model Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ecf9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR_CODE_HERE: Make predictions and evaluate the model\n",
    "y_pred = model.predict(X_test_tfidf)\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e6296c",
   "metadata": {},
   "source": [
    "### Task 5: Ethical Considerations\n",
    "Discuss the following questions:\n",
    "\n",
    "1. What are the potential consequences of false positives and false negatives in fake news detection?\n",
    "2. How might this system be biased based on the training data?\n",
    "3. What are the implications of automated content moderation on free speech?\n",
    "4. How can we ensure transparency and accountability in AI-powered fact-checking systems?\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
