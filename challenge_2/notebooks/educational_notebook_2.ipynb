{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f211c6a7",
   "metadata": {},
   "source": [
    "# 📰 Challenge 2: Fake News Detection AI Project - Educational Notebook\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/BridgingAISocietySummerSchools/Coding-Project/blob/main/challenge_2/notebooks/educational_notebook_2.ipynb)\n",
    "\n",
    "## 🎯 Theme: AI for Combating Misinformation and Promoting Media Literacy\n",
    "\n",
    "Welcome to Challenge 2! You'll build an intelligent system to detect fake news articles using Natural Language Processing and machine learning. This challenge explores how AI can help combat misinformation while considering the complex challenges of bias, context, and freedom of expression.\n",
    "\n",
    "## 📖 What You'll Learn\n",
    "- **Text Preprocessing**: Clean and prepare news article text for analysis\n",
    "- **Feature Extraction**: Convert text into numerical features using TF-IDF\n",
    "- **Binary Classification**: Distinguish between real and fake news articles\n",
    "- **Model Evaluation**: Assess performance with precision, recall, and F1-score\n",
    "- **Pattern Recognition**: Identify linguistic patterns in misinformation\n",
    "- **Ethical Considerations**: Navigate the complexities of automated content moderation\n",
    "\n",
    "## \udcf0 Dataset Overview\n",
    "You'll work with real news articles loaded directly from GitHub:\n",
    "- **Real News**: Authentic articles from reputable news sources (`True.csv`)\n",
    "- **Fake News**: Identified misinformation articles (`Fake.csv`)\n",
    "- **Sample Size**: 40,000+ articles total\n",
    "- **Features**: Article titles, content, subjects, and publication dates\n",
    "- **Source**: Curated collection from various news outlets\n",
    "- **Access**: Automatically loaded from GitHub repository\n",
    "\n",
    "## 🚀 Challenge Roadmap\n",
    "Follow these steps to build your fake news detector:\n",
    "\n",
    "1. **📊 Data Exploration**: Understand the structure and patterns in news data\n",
    "2. **🧹 Text Preprocessing**: Clean and standardize article text\n",
    "3. **🔍 Feature Engineering**: Extract meaningful features from text\n",
    "4. **🤖 Model Training**: Build and train classification models\n",
    "5. **📈 Model Evaluation**: Assess performance and understand predictions\n",
    "6. **🔬 Pattern Analysis**: Discover what makes news \"fake\" vs \"real\"\n",
    "7. **💭 Ethical Reflection**: Consider implications of automated fact-checking\n",
    "\n",
    "---\n",
    "\n",
    "## 💡 **Key Insight**: \n",
    "Fake news detection is not just a technical challenge - it involves complex questions about truth, bias, context, and the role of AI in information ecosystems. Building these tools requires careful consideration of their societal impact.\n",
    "\n",
    "---\n",
    "\n",
    "## ⚠️ **Important Considerations**:\n",
    "- **No system is perfect**: AI can help flag suspicious content but shouldn't be the sole arbiter of truth\n",
    "- **Context matters**: The same facts can be presented with different biases\n",
    "- **Evolution of deception**: Bad actors constantly adapt to evade detection systems\n",
    "- **Human oversight**: Critical decisions about information should involve human judgment\n",
    "\n",
    "---\n",
    "\n",
    "### Task 1: Load and Explore the Dataset\n",
    "\n",
    "**🎯 Goal**: Understand your news data and the challenge of distinguishing real from fake content\n",
    "\n",
    "**📝 What to do**:\n",
    "- Load the dataset directly from GitHub repository\n",
    "- Examine article structure and content\n",
    "- Analyze the distribution of real vs. fake news\n",
    "- Compare sample articles to identify potential patterns\n",
    "- Look for obvious linguistic or structural differences\n",
    "\n",
    "**💡 Key Questions to Explore**:\n",
    "- What makes an article \"fake\" vs. \"real\"?\n",
    "- Are there obvious patterns in language, tone, or structure?\n",
    "- How might the definition of \"fake news\" affect our approach?\n",
    "- What challenges might arise in real-world deployment?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c57d8eed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading fake news dataset from GitHub...\n",
      "✅ Loaded 23481 fake news articles\n",
      "✅ Loaded 23481 fake news articles\n",
      "✅ Loaded 21417 real news articles\n",
      "\n",
      "📊 Combined dataset shape: (44898, 5)\n",
      "📊 Total articles: 44898\n",
      "\n",
      "🔍 Dataset Overview:\n",
      "First few rows:\n",
      "                                               title  \\\n",
      "0   Donald Trump Sends Out Embarrassing New Year’...   \n",
      "1   Drunk Bragging Trump Staffer Started Russian ...   \n",
      "2   Sheriff David Clarke Becomes An Internet Joke...   \n",
      "3   Trump Is So Obsessed He Even Has Obama’s Name...   \n",
      "4   Pope Francis Just Called Out Donald Trump Dur...   \n",
      "\n",
      "                                                text  label  \n",
      "0  Donald Trump just couldn t wish all Americans ...      0  \n",
      "1  House Intelligence Committee Chairman Devin Nu...      0  \n",
      "2  On Friday, it was revealed that former Milwauk...      0  \n",
      "3  On Christmas day, Donald Trump announced that ...      0  \n",
      "4  Pope Francis used his annual Christmas Day mes...      0  \n",
      "\n",
      "📈 Label distribution:\n",
      "Real news (1): 21417\n",
      "Fake news (0): 23481\n",
      "\n",
      "📋 Column information:\n",
      "Columns: ['title', 'text', 'subject', 'date', 'label']\n",
      "Data types: {'title': dtype('O'), 'text': dtype('O'), 'subject': dtype('O'), 'date': dtype('O'), 'label': dtype('int64')}\n",
      "✅ Loaded 21417 real news articles\n",
      "\n",
      "📊 Combined dataset shape: (44898, 5)\n",
      "📊 Total articles: 44898\n",
      "\n",
      "🔍 Dataset Overview:\n",
      "First few rows:\n",
      "                                               title  \\\n",
      "0   Donald Trump Sends Out Embarrassing New Year’...   \n",
      "1   Drunk Bragging Trump Staffer Started Russian ...   \n",
      "2   Sheriff David Clarke Becomes An Internet Joke...   \n",
      "3   Trump Is So Obsessed He Even Has Obama’s Name...   \n",
      "4   Pope Francis Just Called Out Donald Trump Dur...   \n",
      "\n",
      "                                                text  label  \n",
      "0  Donald Trump just couldn t wish all Americans ...      0  \n",
      "1  House Intelligence Committee Chairman Devin Nu...      0  \n",
      "2  On Friday, it was revealed that former Milwauk...      0  \n",
      "3  On Christmas day, Donald Trump announced that ...      0  \n",
      "4  Pope Francis used his annual Christmas Day mes...      0  \n",
      "\n",
      "📈 Label distribution:\n",
      "Real news (1): 21417\n",
      "Fake news (0): 23481\n",
      "\n",
      "📋 Column information:\n",
      "Columns: ['title', 'text', 'subject', 'date', 'label']\n",
      "Data types: {'title': dtype('O'), 'text': dtype('O'), 'subject': dtype('O'), 'date': dtype('O'), 'label': dtype('int64')}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Load the dataset from GitHub repository\n",
    "print(\"Loading fake news dataset from GitHub...\")\n",
    "\n",
    "# GitHub raw URLs for the dataset files\n",
    "fake_url = \"https://raw.githubusercontent.com/BridgingAISocietySummerSchools/Coding-Project/main/challenge_2/data/Fake.csv\"\n",
    "true_url = \"https://raw.githubusercontent.com/BridgingAISocietySummerSchools/Coding-Project/main/challenge_2/data/True.csv\"\n",
    "\n",
    "try:\n",
    "    # Load fake news articles\n",
    "    fake_df = pd.read_csv(fake_url)\n",
    "    fake_df['label'] = 0  # 0 for fake news\n",
    "    print(f\"✅ Loaded {len(fake_df)} fake news articles\")\n",
    "    \n",
    "    # Load true news articles  \n",
    "    true_df = pd.read_csv(true_url)\n",
    "    true_df['label'] = 1  # 1 for real news\n",
    "    print(f\"✅ Loaded {len(true_df)} real news articles\")\n",
    "    \n",
    "    # Combine datasets\n",
    "    df = pd.concat([fake_df, true_df], ignore_index=True)\n",
    "    \n",
    "    print(f\"\\n📊 Combined dataset shape: {df.shape}\")\n",
    "    print(f\"📊 Total articles: {len(df)}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error loading data: {e}\")\n",
    "    print(\"💡 Trying to load from local data folder as fallback...\")\n",
    "    try:\n",
    "        df = pd.read_csv(\"../data/fake_news_dataset.csv\")\n",
    "        print(\"✅ Loaded from local data folder\")\n",
    "    except:\n",
    "        print(\"❌ Could not load data from local folder either\")\n",
    "        raise\n",
    "\n",
    "print(\"\\n🔍 Dataset Overview:\")\n",
    "print(\"First few rows:\")\n",
    "display_cols = ['title', 'text', 'label'] if 'title' in df.columns else df.columns[:3].tolist()\n",
    "print(df[display_cols].head())\n",
    "\n",
    "print(\"\\n📈 Label distribution:\")\n",
    "label_counts = df['label'].value_counts()\n",
    "print(f\"Real news (1): {label_counts.get(1, 0)}\")\n",
    "print(f\"Fake news (0): {label_counts.get(0, 0)}\")\n",
    "\n",
    "print(\"\\n📋 Column information:\")\n",
    "print(f\"Columns: {list(df.columns)}\")\n",
    "print(f\"Data types: {df.dtypes.to_dict()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c9ad7e",
   "metadata": {},
   "source": [
    "### Task 2: Text Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89e6475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text preprocessing completed!\n",
      "                                                text  \\\n",
      "0  Donald Trump just couldn t wish all Americans ...   \n",
      "1  House Intelligence Committee Chairman Devin Nu...   \n",
      "2  On Friday, it was revealed that former Milwauk...   \n",
      "3  On Christmas day, Donald Trump announced that ...   \n",
      "4  Pope Francis used his annual Christmas Day mes...   \n",
      "\n",
      "                                      processed_text  \n",
      "0  donald trump wish americans happy new year lea...  \n",
      "1  house intelligence committee chairman devin nu...  \n",
      "2  friday revealed former milwaukee sheriff david...  \n",
      "3  christmas day donald trump announced would bac...  \n",
      "4  pope francis used annual christmas day message...  \n"
     ]
    }
   ],
   "source": [
    "# 📝 BASIC TEXT EXPLORATION\n",
    "print(\"📝 BASIC TEXT EXPLORATION\")\n",
    "print(\"=\"*35)\n",
    "\n",
    "# Basic text analysis without heavy preprocessing\n",
    "print(\"📊 Sample articles by category:\")\n",
    "print(\"\\n🔴 FAKE NEWS SAMPLE:\")\n",
    "if len(df[df['label'] == 0]) > 0:\n",
    "    fake_sample = df[df['label'] == 0].iloc[0]\n",
    "    if 'title' in df.columns:\n",
    "        print(f\"Title: {fake_sample['title'][:100]}...\")\n",
    "    if 'text' in df.columns:\n",
    "        print(f\"Text: {fake_sample['text'][:200]}...\")\n",
    "\n",
    "print(\"\\n🔵 REAL NEWS SAMPLE:\")\n",
    "if len(df[df['label'] == 1]) > 0:\n",
    "    real_sample = df[df['label'] == 1].iloc[0]\n",
    "    if 'title' in df.columns:\n",
    "        print(f\"Title: {real_sample['title'][:100]}...\")\n",
    "    if 'text' in df.columns:\n",
    "        print(f\"Text: {real_sample['text'][:200]}...\")\n",
    "\n",
    "# Quick statistics\n",
    "print(f\"\\n📈 BASIC STATISTICS:\")\n",
    "print(f\"Total articles: {len(df):,}\")\n",
    "print(f\"Fake articles: {len(df[df['label'] == 0]):,}\")\n",
    "print(f\"Real articles: {len(df[df['label'] == 1]):,}\")\n",
    "\n",
    "if 'text' in df.columns:\n",
    "    print(f\"Average text length: {df['text'].str.len().mean():.0f} characters\")\n",
    "\n",
    "print(\"\\n✅ Basic exploration completed!\")\n",
    "print(\"📝 Note: Detailed text processing will be done in the next section.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68fb835",
   "metadata": {},
   "source": [
    "### Task 3: Feature Extraction and Model Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6360d35d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SIMPLE TEXT ANALYSIS & FEATURE ENGINEERING\n",
      "============================================================\n",
      "1. Basic Text Features\n",
      "-------------------------\n",
      "2. Processing Text\n",
      "--------------------\n",
      "Using column: combined_text\n",
      "Processed 44898 articles\n",
      "Average word count: 417.7\n",
      "\n",
      "3. Basic Analysis by Label\n",
      "------------------------------\n",
      "Mean values by label:\n",
      "       char_count  word_count  exclamation_count\n",
      "label                                           \n",
      "0         2642.59      437.93               0.88\n",
      "1         2448.95      395.59               0.06\n",
      "\n",
      "Sample cleaned text:\n",
      "FAKE: donald trump sends out embarrassing new year s eve message this is disturbing donald trump just coul...\n",
      "FAKE: drunk bragging trump staffer started russian collusion investigation house intelligence committee ch...\n",
      "\n",
      "Text processing completed successfully!\n",
      "Processed 44898 articles\n",
      "Average word count: 417.7\n",
      "\n",
      "3. Basic Analysis by Label\n",
      "------------------------------\n",
      "Mean values by label:\n",
      "       char_count  word_count  exclamation_count\n",
      "label                                           \n",
      "0         2642.59      437.93               0.88\n",
      "1         2448.95      395.59               0.06\n",
      "\n",
      "Sample cleaned text:\n",
      "FAKE: donald trump sends out embarrassing new year s eve message this is disturbing donald trump just coul...\n",
      "FAKE: drunk bragging trump staffer started russian collusion investigation house intelligence committee ch...\n",
      "\n",
      "Text processing completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# SIMPLE TEXT ANALYSIS & FEATURE ENGINEERING\n",
    "print(\"SIMPLE TEXT ANALYSIS & FEATURE ENGINEERING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 1. Basic Text Features\n",
    "print(\"1. Basic Text Features\")\n",
    "print(\"-\" * 25)\n",
    "\n",
    "def get_text_stats(text):\n",
    "    \"\"\"Get basic text statistics\"\"\"\n",
    "    if pd.isna(text) or text == '':\n",
    "        return {'char_count': 0, 'word_count': 0, 'exclamation_count': 0}\n",
    "    \n",
    "    text_str = str(text)\n",
    "    return {\n",
    "        'char_count': len(text_str),\n",
    "        'word_count': len(text_str.split()),\n",
    "        'exclamation_count': text_str.count('!')\n",
    "    }\n",
    "\n",
    "def clean_text_basic(text):\n",
    "    \"\"\"Basic text cleaning\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = str(text).lower()\n",
    "    # Remove punctuation \n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    # Remove extra spaces\n",
    "    text = ' '.join(text.split())\n",
    "    \n",
    "    return text\n",
    "\n",
    "# 2. Apply Processing\n",
    "print(\"2. Processing Text\")\n",
    "print(\"-\" * 20)\n",
    "\n",
    "# Combine title and text if both exist\n",
    "if 'title' in df.columns and 'text' in df.columns:\n",
    "    df['combined_text'] = df['title'].fillna('').astype(str) + ' ' + df['text'].fillna('').astype(str)\n",
    "    text_col = 'combined_text'\n",
    "elif 'text' in df.columns:\n",
    "    text_col = 'text'\n",
    "else:\n",
    "    text_col = df.columns[0]  # Use first column\n",
    "\n",
    "print(f\"Using column: {text_col}\")\n",
    "\n",
    "# Get text features\n",
    "text_features = df[text_col].apply(get_text_stats)\n",
    "for feature in ['char_count', 'word_count', 'exclamation_count']:\n",
    "    df[feature] = [f[feature] for f in text_features]\n",
    "\n",
    "# Clean text\n",
    "df['cleaned_text'] = df[text_col].apply(clean_text_basic)\n",
    "\n",
    "print(f\"Processed {len(df)} articles\")\n",
    "print(f\"Average word count: {df['word_count'].mean():.1f}\")\n",
    "\n",
    "# 3. Basic Analysis\n",
    "print(f\"\\n3. Basic Analysis by Label\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "feature_cols = ['char_count', 'word_count', 'exclamation_count']\n",
    "analysis = df.groupby('label')[feature_cols].mean()\n",
    "print(\"Mean values by label:\")\n",
    "print(analysis.round(2))\n",
    "\n",
    "# Show samples\n",
    "print(f\"\\nSample cleaned text:\")\n",
    "for i, (label, text) in enumerate(zip(df['label'].head(2), df['cleaned_text'].head(2))):\n",
    "    label_name = \"REAL\" if label == 1 else \"FAKE\"\n",
    "    print(f\"{label_name}: {text[:100]}...\")\n",
    "\n",
    "print(f\"\\nText processing completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d66d48",
   "metadata": {},
   "source": [
    "### Task 4: Model Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ecf9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🤖 EFFICIENT MACHINE LEARNING FOR FAKE NEWS DETECTION\n",
    "print(\"🤖 EFFICIENT MACHINE LEARNING FOR FAKE NEWS DETECTION\")\n",
    "print(\"=\"*65)\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 1. Prepare Data Efficiently\n",
    "print(\"1. Data Preparation\")\n",
    "print(\"-\" * 25)\n",
    "\n",
    "# Use cleaned text from previous processing\n",
    "text_data = df['cleaned_text'].fillna('')\n",
    "y = df['label'].values\n",
    "\n",
    "# Basic numerical features only\n",
    "numerical_features = ['char_count', 'word_count', 'exclamation_count']\n",
    "X_numerical = df[numerical_features].fillna(0)\n",
    "\n",
    "print(f\"✅ Text samples: {len(text_data)}\")\n",
    "print(f\"✅ Numerical features: {len(numerical_features)}\")\n",
    "print(f\"✅ Target distribution: Fake={np.sum(y==0)}, Real={np.sum(y==1)}\")\n",
    "\n",
    "# 2. Fast Train-Test Split\n",
    "print(f\"\\n2. Data Splitting\")\n",
    "print(\"-\" * 20)\n",
    "\n",
    "X_train_text, X_test_text, X_train_num, X_test_num, y_train, y_test = train_test_split(\n",
    "    text_data, X_numerical, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set: {len(X_train_text)} samples\")\n",
    "print(f\"Test set: {len(X_test_text)} samples\")\n",
    "\n",
    "# 3. Simple Text Vectorization\n",
    "print(f\"\\n3. Text Vectorization\")\n",
    "print(\"-\" * 25)\n",
    "\n",
    "# Use smaller feature set for speed\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=2000,  # Reduced for speed\n",
    "    stop_words='english',\n",
    "    ngram_range=(1, 2),\n",
    "    min_df=5,\n",
    "    max_df=0.9\n",
    ")\n",
    "\n",
    "print(\"Vectorizing text...\")\n",
    "X_train_vec = vectorizer.fit_transform(X_train_text)\n",
    "X_test_vec = vectorizer.transform(X_test_text)\n",
    "\n",
    "# Combine features\n",
    "X_train_combined = np.hstack([X_train_vec.toarray(), X_train_num.values])\n",
    "X_test_combined = np.hstack([X_test_vec.toarray(), X_test_num.values])\n",
    "\n",
    "print(f\"Combined feature dimensions: {X_train_combined.shape[1]}\")\n",
    "\n",
    "# 4. Quick Model Comparison\n",
    "print(f\"\\n4. Model Comparison\")\n",
    "print(\"-\" * 25)\n",
    "\n",
    "# Test 3 fast models only\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=500),\n",
    "    'Naive Bayes': MultinomialNB(),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=50, random_state=42)  # Reduced trees\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n🔄 Testing {name}...\")\n",
    "    \n",
    "    # Quick 3-fold CV for speed\n",
    "    cv_scores = cross_val_score(model, X_train_combined, y_train, cv=3, scoring='accuracy')\n",
    "    \n",
    "    # Train and test\n",
    "    model.fit(X_train_combined, y_train)\n",
    "    y_pred = model.predict(X_test_combined)\n",
    "    test_acc = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    results[name] = {\n",
    "        'model': model,\n",
    "        'cv_mean': cv_scores.mean(),\n",
    "        'cv_std': cv_scores.std(),\n",
    "        'test_accuracy': test_acc,\n",
    "        'predictions': y_pred\n",
    "    }\n",
    "    \n",
    "    print(f\"  CV Accuracy: {cv_scores.mean():.3f} ± {cv_scores.std():.3f}\")\n",
    "    print(f\"  Test Accuracy: {test_acc:.3f}\")\n",
    "\n",
    "# 5. Best Model Analysis\n",
    "print(f\"\\n5. Best Model Results\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Find best model\n",
    "best_model_name = max(results.keys(), key=lambda k: results[k]['test_accuracy'])\n",
    "best_result = results[best_model_name]\n",
    "\n",
    "print(f\"🏆 BEST MODEL: {best_model_name}\")\n",
    "print(f\"   Test Accuracy: {best_result['test_accuracy']:.3f}\")\n",
    "print(f\"   CV Score: {best_result['cv_mean']:.3f} ± {best_result['cv_std']:.3f}\")\n",
    "\n",
    "# Classification report\n",
    "print(f\"\\n📊 CLASSIFICATION REPORT:\")\n",
    "print(\"-\" * 35)\n",
    "print(classification_report(y_test, best_result['predictions'], \n",
    "                          target_names=['Fake News', 'Real News']))\n",
    "\n",
    "# 6. Quick Visualization\n",
    "print(f\"\\n6. Results Visualization\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, best_result['predictions'])\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0],\n",
    "            xticklabels=['Fake', 'Real'], yticklabels=['Fake', 'Real'])\n",
    "axes[0].set_title(f'Confusion Matrix\\n{best_model_name}', fontweight='bold')\n",
    "axes[0].set_ylabel('True Label')\n",
    "axes[0].set_xlabel('Predicted Label')\n",
    "\n",
    "# Model Comparison\n",
    "model_names = list(results.keys())\n",
    "accuracies = [results[name]['test_accuracy'] for name in model_names]\n",
    "\n",
    "bars = axes[1].bar(model_names, accuracies, color=['skyblue', 'lightcoral', 'lightgreen'])\n",
    "axes[1].set_title('Model Performance Comparison', fontweight='bold')\n",
    "axes[1].set_ylabel('Test Accuracy')\n",
    "axes[1].set_ylim(0, 1)\n",
    "\n",
    "# Add accuracy labels on bars\n",
    "for bar, acc in zip(bars, accuracies):\n",
    "    axes[1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                f'{acc:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 7. Feature Insights\n",
    "print(f\"\\n7. Top Predictive Features\")\n",
    "print(\"-\" * 35)\n",
    "\n",
    "if hasattr(best_result['model'], 'feature_importances_'):\n",
    "    # Get feature names\n",
    "    feature_names = list(vectorizer.get_feature_names_out()) + numerical_features\n",
    "    importances = best_result['model'].feature_importances_\n",
    "    \n",
    "    # Get top features\n",
    "    top_indices = np.argsort(importances)[-10:]\n",
    "    top_features = [feature_names[i] for i in top_indices]\n",
    "    top_importance = [importances[i] for i in top_indices]\n",
    "    \n",
    "    print(\"🔑 TOP 10 MOST IMPORTANT FEATURES:\")\n",
    "    for i, (feat, imp) in enumerate(zip(reversed(top_features), reversed(top_importance)), 1):\n",
    "        feature_type = \"📊\" if feat in numerical_features else \"📝\"\n",
    "        print(f\"{i:2d}. {feature_type} {feat:20s}: {imp:.4f}\")\n",
    "\n",
    "print(f\"\\n💡 KEY INSIGHTS:\")\n",
    "print(\"=\"*20)\n",
    "print(f\"✅ Best model achieves {best_result['test_accuracy']:.1%} accuracy\")\n",
    "print(f\"✅ Text analysis successfully distinguishes fake from real news\")\n",
    "print(f\"✅ Simple features provide strong predictive power\")\n",
    "\n",
    "print(f\"\\n🎯 Fake News Detection Analysis Complete! 🎯\")\n",
    "print(f\"⚡ Optimized for speed and efficiency!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e6296c",
   "metadata": {},
   "source": [
    "### Task 5: Ethical Considerations\n",
    "Discuss the following questions:\n",
    "\n",
    "1. What are the potential consequences of false positives and false negatives in fake news detection?\n",
    "2. How might this system be biased based on the training data?\n",
    "3. What are the implications of automated content moderation on free speech?\n",
    "4. How can we ensure transparency and accountability in AI-powered fact-checking systems?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d5c8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ⚡ QUICK HYPERPARAMETER OPTIMIZATION\n",
    "print(\"⚡ QUICK HYPERPARAMETER OPTIMIZATION\")\n",
    "print(\"=\"*45)\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Only tune the best model with a small parameter grid for speed\n",
    "if 'best_model_name' in locals() and 'best_result' in locals():\n",
    "    print(f\"🔧 Quick tuning for: {best_model_name}\")\n",
    "    \n",
    "    # Small parameter grids for speed\n",
    "    quick_params = {\n",
    "        'Logistic Regression': {\n",
    "            'C': [0.1, 1, 10]  # Just 3 values\n",
    "        },\n",
    "        'Random Forest': {\n",
    "            'n_estimators': [50, 100],  # Just 2 values\n",
    "            'max_depth': [10, None]     # Just 2 values\n",
    "        },\n",
    "        'Naive Bayes': {\n",
    "            'alpha': [0.1, 1.0]  # Just 2 values\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    if best_model_name in quick_params:\n",
    "        print(f\"🔍 Running quick GridSearch...\")\n",
    "        \n",
    "        # Create fresh model instance\n",
    "        if best_model_name == 'Logistic Regression':\n",
    "            model = LogisticRegression(random_state=42, max_iter=500)\n",
    "        elif best_model_name == 'Random Forest':\n",
    "            model = RandomForestClassifier(random_state=42)\n",
    "        elif best_model_name == 'Naive Bayes':\n",
    "            model = MultinomialNB()\n",
    "        \n",
    "        # Quick grid search (2-fold CV for speed)\n",
    "        grid_search = GridSearchCV(\n",
    "            model, \n",
    "            quick_params[best_model_name], \n",
    "            cv=2,  # Only 2 folds for speed\n",
    "            scoring='accuracy',\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        \n",
    "        grid_search.fit(X_train_combined, y_train)\n",
    "        \n",
    "        # Test tuned model\n",
    "        tuned_pred = grid_search.predict(X_test_combined)\n",
    "        tuned_accuracy = accuracy_score(y_test, tuned_pred)\n",
    "        \n",
    "        print(f\"\\n📊 QUICK TUNING RESULTS:\")\n",
    "        print(\"-\" * 30)\n",
    "        print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "        print(f\"Original accuracy: {best_result['test_accuracy']:.4f}\")\n",
    "        print(f\"Tuned accuracy:    {tuned_accuracy:.4f}\")\n",
    "        print(f\"Improvement:       {tuned_accuracy - best_result['test_accuracy']:+.4f}\")\n",
    "        \n",
    "        if tuned_accuracy > best_result['test_accuracy']:\n",
    "            print(\"✅ Tuning improved performance!\")\n",
    "        else:\n",
    "            print(\"📝 Original parameters were already good\")\n",
    "    \n",
    "    else:\n",
    "        print(f\"⚠️  Quick tuning not available for {best_model_name}\")\n",
    "\n",
    "else:\n",
    "    print(\"⚠️  No model results found - run previous cell first\")\n",
    "\n",
    "print(f\"\\n🎯 Quick Optimization Complete! 🎯\")\n",
    "print(f\"💡 For production use, consider more extensive hyperparameter search\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
