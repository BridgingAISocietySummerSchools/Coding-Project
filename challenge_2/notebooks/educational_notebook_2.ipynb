{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f211c6a7",
   "metadata": {},
   "source": [
    "# Challenge 2: Fake News Detector Project - Educational Notebook\n",
    "\n",
    "## üéØ Theme: AI for Information Integrity and Media Literacy\n",
    "\n",
    "Welcome to Challenge 2! You'll build an intelligent fake news detection system that helps identify misinformation in news articles. This challenge explores one of the most critical applications of AI in our information age - combating the spread of false information.\n",
    "\n",
    "## üìñ What You'll Learn\n",
    "- **Text Classification**: Distinguish between real and fake news articles\n",
    "- **Feature Engineering**: Extract meaningful signals from news text\n",
    "- **NLP Techniques**: Apply advanced text processing for classification\n",
    "- **Model Evaluation**: Assess classifier performance for critical applications\n",
    "- **Information Literacy**: Understand how misinformation spreads and can be detected\n",
    "- **Ethical AI**: Consider the responsibility and challenges of automated fact-checking\n",
    "\n",
    "## üóÇÔ∏è Dataset Overview\n",
    "You'll work with a news article dataset containing:\n",
    "- **Headlines**: Article titles that may contain bias or sensational language\n",
    "- **Text Content**: Full article text with varying writing styles and quality\n",
    "- **Labels**: Binary classification (real=1, fake=0)\n",
    "- **Diverse Sources**: Different types of news sources and topics\n",
    "\n",
    "## üöÄ Challenge Roadmap\n",
    "Follow these steps to build your fake news detector:\n",
    "\n",
    "1. **üìä Data Exploration**: Understand news article patterns and distributions\n",
    "2. **üîç Linguistic Analysis**: Discover differences between real and fake news\n",
    "3. **üßπ Text Preprocessing**: Clean and standardize news content\n",
    "4. **‚öôÔ∏è Feature Engineering**: Extract signals that distinguish fake from real news\n",
    "5. **ü§ñ Model Training**: Build and train a news classifier\n",
    "6. **üìà Evaluation**: Assess model performance with appropriate metrics\n",
    "7. **üí≠ Critical Thinking**: Consider limitations and ethical implications\n",
    "\n",
    "---\n",
    "\n",
    "## üí° **Key Insight**: \n",
    "Fake news detection goes beyond simple keyword matching - it requires understanding linguistic patterns, source credibility, and the subtle ways misinformation can be crafted to appear legitimate.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ö†Ô∏è **Important Considerations**:\n",
    "- **No system is perfect**: AI can help flag suspicious content but shouldn't be the sole arbiter of truth\n",
    "- **Context matters**: The same facts can be presented with different biases\n",
    "- **Evolution of deception**: Bad actors constantly adapt to evade detection systems\n",
    "- **Human oversight**: Critical decisions about information should involve human judgment\n",
    "\n",
    "---\n",
    "\n",
    "### Task 1: Load and Explore the Dataset\n",
    "\n",
    "**üéØ Goal**: Understand your news data and the challenge of distinguishing real from fake content\n",
    "\n",
    "**üìù What to do**:\n",
    "- Load the dataset and examine article structure\n",
    "- Analyze the distribution of real vs. fake news\n",
    "- Compare sample articles to identify potential patterns\n",
    "- Look for obvious linguistic or structural differences\n",
    "\n",
    "**üí° Key Questions to Explore**:\n",
    "- What makes an article \"fake\" vs. \"real\"?\n",
    "- Are there obvious patterns in language, tone, or structure?\n",
    "- How might the definition of \"fake news\" affect our approach?\n",
    "- What challenges might arise in real-world deployment?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57d8eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Load the dataset from GitHub repository\n",
    "print(\"Loading fake news dataset from GitHub...\")\n",
    "\n",
    "# GitHub raw URLs for the dataset files\n",
    "fake_url = \"https://raw.githubusercontent.com/BridgingAISocietySummerSchools/Coding-Project/main/challenge_2/data/Fake.csv\"\n",
    "true_url = \"https://raw.githubusercontent.com/BridgingAISocietySummerSchools/Coding-Project/main/challenge_2/data/True.csv\"\n",
    "\n",
    "try:\n",
    "    # Load fake news articles\n",
    "    fake_df = pd.read_csv(fake_url)\n",
    "    fake_df['label'] = 0  # 0 for fake news\n",
    "    print(f\"‚úÖ Loaded {len(fake_df)} fake news articles\")\n",
    "    \n",
    "    # Load true news articles  \n",
    "    true_df = pd.read_csv(true_url)\n",
    "    true_df['label'] = 1  # 1 for real news\n",
    "    print(f\"‚úÖ Loaded {len(true_df)} real news articles\")\n",
    "    \n",
    "    # Combine datasets\n",
    "    df = pd.concat([fake_df, true_df], ignore_index=True)\n",
    "    \n",
    "    print(f\"\\nüìä Combined dataset shape: {df.shape}\")\n",
    "    print(f\"üìä Total articles: {len(df)}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading data: {e}\")\n",
    "    print(\"üí° Trying to load from local data folder as fallback...\")\n",
    "    try:\n",
    "        df = pd.read_csv(\"../data/fake_news_dataset.csv\")\n",
    "        print(\"‚úÖ Loaded from local data folder\")\n",
    "    except:\n",
    "        print(\"‚ùå Could not load data from local folder either\")\n",
    "        raise\n",
    "\n",
    "print(\"\\nüîç Dataset Overview:\")\n",
    "print(\"First few rows:\")\n",
    "display_cols = ['title', 'text', 'label'] if 'title' in df.columns else df.columns[:3].tolist()\n",
    "print(df[display_cols].head())\n",
    "\n",
    "print(\"\\nüìà Label distribution:\")\n",
    "label_counts = df['label'].value_counts()\n",
    "print(f\"Real news (1): {label_counts.get(1, 0)}\")\n",
    "print(f\"Fake news (0): {label_counts.get(0, 0)}\")\n",
    "\n",
    "print(\"\\nüìã Column information:\")\n",
    "print(f\"Columns: {list(df.columns)}\")\n",
    "print(f\"Data types: {df.dtypes.to_dict()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c9ad7e",
   "metadata": {},
   "source": [
    "### Task 2: Text Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89e6475",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download required NLTK data\n",
    "try:\n",
    "    stopwords.words('english')\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove special characters and digits\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    # Remove stopwords\n",
    "    text = ' '.join([word for word in text.split() if word not in stop_words])\n",
    "    return text\n",
    "\n",
    "# YOUR_CODE_HERE: Apply preprocessing to the text column\n",
    "df['processed_text'] = df['text'].apply(preprocess_text)\n",
    "\n",
    "print(\"Text preprocessing completed!\")\n",
    "print(df[['text', 'processed_text']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68fb835",
   "metadata": {},
   "source": [
    "### Task 3: Feature Extraction and Model Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6360d35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR_CODE_HERE: Split the data into training and testing sets\n",
    "X = df['processed_text']\n",
    "y = df['label']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# YOUR_CODE_HERE: Create TF-IDF features\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "# YOUR_CODE_HERE: Train a Naive Bayes classifier\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "print(\"Model training completed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d66d48",
   "metadata": {},
   "source": [
    "### Task 4: Model Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ecf9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR_CODE_HERE: Make predictions and evaluate the model\n",
    "y_pred = model.predict(X_test_tfidf)\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(\"\n",
    "Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e6296c",
   "metadata": {},
   "source": [
    "### Task 5: Ethical Considerations\n",
    "Discuss the following questions:\n",
    "\n",
    "1. What are the potential consequences of false positives and false negatives in fake news detection?\n",
    "2. How might this system be biased based on the training data?\n",
    "3. What are the implications of automated content moderation on free speech?\n",
    "4. How can we ensure transparency and accountability in AI-powered fact-checking systems?\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
